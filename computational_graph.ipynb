{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook demonstrates how to extract and visualize computational graphs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import onnx\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting pytorch model to onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pytorch_to_onnx(model_name, output_path, input_shape=None, text_example=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    if text_example:\n",
    "        inputs = tokenizer(text_example, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "    elif input_shape:\n",
    "        batch_size, seq_length = input_shape\n",
    "        input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n",
    "        attention_mask = torch.ones(batch_size, seq_length)\n",
    "    else:\n",
    "        batch_size, seq_length = 1, 16\n",
    "        input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_length))\n",
    "        attention_mask = torch.ones(batch_size, seq_length)\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        model,                                         # PyTorch model\n",
    "        (input_ids, attention_mask),                   # Model input\n",
    "        output_path,                                   # Output file\n",
    "        export_params=True,                            # Store the trained weights\n",
    "        opset_version=14,                              # ONNX version\n",
    "        do_constant_folding=True,                      # Optimization\n",
    "        input_names=[\"input_ids\", \"attention_mask\"],   # Input names\n",
    "        output_names=[\"last_hidden_state\"],            # Output names\n",
    "        dynamic_axes={                                  # Dynamic axes\n",
    "            \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "            \"last_hidden_state\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to {output_path}\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33ed3c7b8d94c5da974935de8e7805a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210a858a47dc425497c028aa8a66540d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6645493780f04bbca26bdf2b8769c973",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db458483631b46cba36101ef05b748c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a5bb276cb549399cf41d004737e42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model exported to bert_model.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'bert_model.onnx'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "onnx_path = \"bert_model.onnx\"\n",
    "text_input = \"This is a sample text to analyze the model's computational graph.\"\n",
    "\n",
    "convert_pytorch_to_onnx(model_name, onnx_path, text_example=text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using onnx_model.graph you can acces the computational graph of the model.\n",
    "\n",
    "def analyze_onnx_model(onnx_path):\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    \n",
    "    print(f\"Model IR version: {onnx_model.ir_version}\")\n",
    "    print(f\"Producer name: {onnx_model.producer_name}\")\n",
    "    print(f\"Producer version: {onnx_model.producer_version}\")\n",
    "    print(f\"Model version: {onnx_model.model_version}\")\n",
    "    \n",
    "    op_types = {}\n",
    "    for node in onnx_model.graph.node:\n",
    "        op_type = node.op_type\n",
    "        op_types[op_type] = op_types.get(op_type, 0) + 1\n",
    "    \n",
    "    print(\"\\nOperation types:\")\n",
    "    for op_type, count in sorted(op_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {op_type}: {count}\")\n",
    "    \n",
    "    print(\"\\nInputs:\")\n",
    "    for inp in onnx_model.graph.input:\n",
    "        print(f\"  {inp.name}\")\n",
    "    \n",
    "    print(\"\\nOutputs:\")\n",
    "    for outp in onnx_model.graph.output:\n",
    "        print(f\"  {outp.name}\")\n",
    "    \n",
    "    print(\"\\nModel structure:\")\n",
    "    for i, node in enumerate(onnx_model.graph.node[:20]):  # Print first 20 nodes\n",
    "        print(f\"  Node {i}: {node.op_type} - {node.name}\")\n",
    "        print(f\"    Inputs: {', '.join(node.input)}\")\n",
    "        print(f\"    Outputs: {', '.join(node.output)}\")\n",
    "    \n",
    "    if len(onnx_model.graph.node) > 20:\n",
    "        print(f\"  ... and {len(onnx_model.graph.node) - 20} more nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model IR version: 7\n",
      "Producer name: pytorch\n",
      "Producer version: 2.6.0\n",
      "Model version: 0\n",
      "\n",
      "Operation types:\n",
      "  Constant: 427\n",
      "  Add: 172\n",
      "  Shape: 114\n",
      "  Unsqueeze: 105\n",
      "  Gather: 104\n",
      "  MatMul: 96\n",
      "  Mul: 75\n",
      "  Sqrt: 61\n",
      "  Concat: 50\n",
      "  Reshape: 50\n",
      "  ReduceMean: 50\n",
      "  Div: 49\n",
      "  Transpose: 48\n",
      "  Cast: 27\n",
      "  Sub: 26\n",
      "  Pow: 25\n",
      "  Slice: 14\n",
      "  Softmax: 12\n",
      "  Erf: 12\n",
      "  Where: 3\n",
      "  ConstantOfShape: 2\n",
      "  Equal: 2\n",
      "  Expand: 2\n",
      "  Gemm: 1\n",
      "  Tanh: 1\n",
      "\n",
      "Inputs:\n",
      "  input_ids\n",
      "  attention_mask\n",
      "\n",
      "Outputs:\n",
      "  last_hidden_state\n",
      "  1895\n",
      "\n",
      "Model structure:\n",
      "  Node 0: Constant - /Constant\n",
      "    Inputs: \n",
      "    Outputs: /Constant_output_0\n",
      "  Node 1: Shape - /Shape\n",
      "    Inputs: input_ids\n",
      "    Outputs: /Shape_output_0\n",
      "  Node 2: Constant - /Constant_1\n",
      "    Inputs: \n",
      "    Outputs: /Constant_1_output_0\n",
      "  Node 3: Gather - /Gather\n",
      "    Inputs: /Shape_output_0, /Constant_1_output_0\n",
      "    Outputs: /Gather_output_0\n",
      "  Node 4: Shape - /Shape_1\n",
      "    Inputs: input_ids\n",
      "    Outputs: /Shape_1_output_0\n",
      "  Node 5: Constant - /Constant_2\n",
      "    Inputs: \n",
      "    Outputs: /Constant_2_output_0\n",
      "  Node 6: Gather - /Gather_1\n",
      "    Inputs: /Shape_1_output_0, /Constant_2_output_0\n",
      "    Outputs: /Gather_1_output_0\n",
      "  Node 7: Constant - Constant_434\n",
      "    Inputs: \n",
      "    Outputs: onnx::Slice_209\n",
      "  Node 8: Constant - /Constant_3\n",
      "    Inputs: \n",
      "    Outputs: /Constant_3_output_0\n",
      "  Node 9: Constant - /Constant_4\n",
      "    Inputs: \n",
      "    Outputs: /Constant_4_output_0\n",
      "  Node 10: Constant - /Constant_5\n",
      "    Inputs: \n",
      "    Outputs: /Constant_5_output_0\n",
      "  Node 11: Unsqueeze - /Unsqueeze\n",
      "    Inputs: /Gather_1_output_0, /Constant_5_output_0\n",
      "    Outputs: /Unsqueeze_output_0\n",
      "  Node 12: Constant - /Constant_6\n",
      "    Inputs: \n",
      "    Outputs: /Constant_6_output_0\n",
      "  Node 13: Slice - /Slice\n",
      "    Inputs: onnx::Slice_209, /Constant_4_output_0, /Unsqueeze_output_0, /Constant_3_output_0, /Constant_6_output_0\n",
      "    Outputs: /Slice_output_0\n",
      "  Node 14: Constant - /Constant_7\n",
      "    Inputs: \n",
      "    Outputs: /Constant_7_output_0\n",
      "  Node 15: Unsqueeze - /Unsqueeze_1\n",
      "    Inputs: /Gather_output_0, /Constant_7_output_0\n",
      "    Outputs: /Unsqueeze_1_output_0\n",
      "  Node 16: Constant - /Constant_8\n",
      "    Inputs: \n",
      "    Outputs: /Constant_8_output_0\n",
      "  Node 17: Unsqueeze - /Unsqueeze_2\n",
      "    Inputs: /Gather_1_output_0, /Constant_8_output_0\n",
      "    Outputs: /Unsqueeze_2_output_0\n",
      "  Node 18: Concat - /Concat\n",
      "    Inputs: /Unsqueeze_1_output_0, /Unsqueeze_2_output_0\n",
      "    Outputs: /Concat_output_0\n",
      "  Node 19: Constant - /Constant_9\n",
      "    Inputs: \n",
      "    Outputs: /Constant_9_output_0\n",
      "  ... and 1508 more nodes\n"
     ]
    }
   ],
   "source": [
    "onnx_path = \"bert_model.onnx\"\n",
    "analyze_onnx_model(onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting subgraphs with specific operators involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_subgraph(onnx_path, target_ops=None, output_path=\"onnx_subgraph.png\", viz=False):\n",
    "    if target_ops is None:\n",
    "        target_ops = [\"Attention\", \"MatMul\", \"Add\"]\n",
    "    \n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    \n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for i, node in enumerate(onnx_model.graph.node):\n",
    "        op_type = node.op_type\n",
    "        if op_type in target_ops:\n",
    "            node_name = node.name or f\"{op_type}_{i}\"\n",
    "            \n",
    "            G.add_node(node_name, op_type=op_type, shape=\"box\", style=\"filled\", color=\"lightblue\")\n",
    "            \n",
    "            for input_name in node.input:\n",
    "                input_base = input_name.split(\".\")[0] if \".\" in input_name else input_name\n",
    "                G.add_edge(input_base, node_name)\n",
    "            \n",
    "            for output_name in node.output:\n",
    "                output_base = output_name.split(\".\")[0] if \".\" in output_name else output_name\n",
    "                G.add_edge(node_name, output_base)\n",
    "    \n",
    "    if viz:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        pos = nx.spring_layout(G, k=0.5)\n",
    "        \n",
    "        nx.draw(\n",
    "            G, pos, with_labels=True, arrows=True, \n",
    "            node_color=\"lightgreen\", node_size=2000, \n",
    "            font_size=8, font_weight=\"bold\", \n",
    "            edge_color=\"gray\", width=0.5\n",
    "        )\n",
    "        \n",
    "        plt.savefig(output_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"Subgraph visualization saved to {output_path}\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!python ./net_drawer.py --input ./bert_model.onnx --output bert_model.dot --embed_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Open svg file in your browser\n",
    "!dot -Tsvg ./bert_model.dot -o bert_model.svg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
